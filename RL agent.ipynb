{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushdeep-singh/Khushdeep-robotics/blob/master/RL%20agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrlVE5N3jvkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as T \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim \n",
        "import numpy as np\n",
        "import gym \n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqobmt-Ej7o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self,alpha):\n",
        "        super(DQN,self).__init__()\n",
        "        \n",
        "        # We define two networks \n",
        "        \n",
        "        #define convolutional networks with dfferent inputs and outputs\n",
        "        self.conv1 = nn.Conv2d(1,32,8, stride =4, padding =1)\n",
        "        self.conv2 = nn.Conv2d(32,64,4, stride =2)\n",
        "        self.conv3 = nn.Conv2d(64,128, 3)\n",
        "        \n",
        "        #initialize fully connected networks \n",
        "        self.fc1 = nn.Linear(128*19*8, 512) # 128*19*8 is a number from users' experience , 512 is ouput \n",
        "        self.fc2 = nn.Linear(512,6)  # 512 as input and 6 as output ( because the actions are 6 in this specific game) \n",
        "        \n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr = alpha)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        #send the network to the device \n",
        "        self.to(self.device) \n",
        "     \n",
        "    # Feed forwarding the network \n",
        "    def forward(self, observation):\n",
        "        # observations are sequence of frames, we can trancate the frame to meet reduced memory reqd.\n",
        "        # using 3 frames to get sense of motion and then converting them to Tensors\n",
        "        observation = T.Tensor(observation).to(self.device) # sending converted observation tensors to device\n",
        "        # re-size the array\n",
        "        observation = observation.view(-1, 1, 185, 95)\n",
        "        # activate and feed forward\n",
        "        observation = F.relu(self.conv1(observation))\n",
        "        observation = F.relu(self.conv2(observation))\n",
        "        observation = F.relu(self.conv3(observation))\n",
        " \n",
        "        # Take convolved images and flatten them to feed into fully connected neural networks \n",
        "        observation = observation.view(-1, 128*19*8)\n",
        "        # Activate and feed forward\n",
        "        observation = F.relu(self.fc1(observation))\n",
        "\n",
        "        actions = F.relu(self.fc2(observation))\n",
        "\n",
        "        return actions \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__DTFOpFj_fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "  def __init__(self, gamma, epsilon, alpha,\n",
        "               maxMemorysize, epsEnd= 0.05,\n",
        "               replace = 10000, actionSpace = [0,1,2,3,4,5]):\n",
        "    # defining the agent parameters\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon \n",
        "    self.epsEnd = epsEnd\n",
        "    self.actionSpace = actionSpace\n",
        "    self.memSize = maxMemorysize\n",
        "    self.steps = 0 \n",
        "    self.learnstepcounter = 0 \n",
        "    self.memory = []\n",
        "    self.memcntr =  0\n",
        "    self.replacetargetcntr = replace\n",
        "    self.Qeval = DQN(alpha)\n",
        "    self.Qnext = DQN(alpha)\n",
        "\n",
        "  def storeTransition(self, state, action, reward, state_):\n",
        "    # storing up the stack of SARS\n",
        "    if self.memcntr < self.memSize:\n",
        "      self.memory.append([state, action, reward, state_])\n",
        "    else:\n",
        "      self.memory[self.memcntr%self.memSize] = [state, action, reward, state_]\n",
        "    \n",
        "    self.memcntr += 1\n",
        "\n",
        "  def chooseAction(self, observation):\n",
        "    # choosing an agent's actions \n",
        "    rand = np.random.random()\n",
        "    # forward propagate stack of frames through cnn and fully connected nn to get some set of states\n",
        "    actions = self.Qeval.forward(observation)\n",
        "\n",
        "    if rand < 1- self.epsilon:\n",
        "      action = T.argmax(actions[1]).item()\n",
        "    else:\n",
        "      action = np.random.choice(self.actionSpace) # choose randomly \n",
        "    \n",
        "    self.steps += 1\n",
        "\n",
        "    return action\n",
        "\n",
        "  def learn(self,batchsize):\n",
        "    # we do batch learning to avoid correlation between state tansitions \n",
        "    self.Qeval.optimizer.zero_grad() # zero the gradients after every batch\n",
        "\n",
        "    if self.replacetargetcntr is not None and (self.learnstepcounter % self.replacetargetcntr == 0):\n",
        "      # convert into state dictionary \n",
        "      self.Qnext.load_state_dict(self.Qeval.state_dict()) \n",
        "\n",
        "    # start of memory sub sampling \n",
        "    if self.memcntr + batchsize < self.memSize:\n",
        "      memStart = int(np.random.choice(range(self.memcntr)))\n",
        "    else:\n",
        "      memStart = int(np.random.choice(range(self.memSize - batchsize - 1)))  \n",
        "\n",
        "    # sample the batch of memory and convert to numpy array \n",
        "    miniBatch = self.memory[memStart:memStart + batchsize ]\n",
        "    memory = np.array(miniBatch)\n",
        "\n",
        "  # convert to list because memory is an array of numpy objects \n",
        "  # feed forwarding the current and successor state \n",
        "\n",
        "    Qpred = self.Qeval.forward(list(memory[:,0][:])).to(self.Qeval.device)\n",
        "    Qnext = self.Qnext.forward(list(memory[:,3][:])).to(self.Qeval.device)\n",
        "\n",
        "    # get the max action \n",
        "    maxA = T.argmax(Qnext, dim=1).to(self.Qeval.device)\n",
        "    # get the reward form 2nd memory element \n",
        "    rewards = T.Tensor(list(memory[:,2])).to(self.Qeval.device)\n",
        "\n",
        "    Qtarget = Qpred.clone()\n",
        "    indices = np.arange(batchsize)\n",
        "    Qtarget[indices,maxA] = rewards + self.gamma*T.max(Qnext[1])\n",
        "\n",
        "    # for converging to small value \n",
        "    if self.steps > 500:\n",
        "        if self.epsilon - 1e-4 > self.epsEnd:\n",
        "            self.epsilon -= 1e-4\n",
        "        else:\n",
        "            self.epsilon = self.epsEnd\n",
        "\n",
        "    #Qpred.requires_grad_()\n",
        "    loss = self.Qeval.loss(Qtarget, Qpred).to(self.Qeval.device) # calculate loss\n",
        "    loss.backward()   # back propagate \n",
        "    self.Qeval.optimizer.step() # optimize \n",
        "    self.learnstepcounter += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geH8ksPO0XIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    #ax2.xaxis.tick_top()\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    #ax2.xaxis.set_label_position('top')\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)\n",
        "\n",
        "class SkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(SkipEnv, self).__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            t_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer = []\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class PreProcessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(PreProcessFrame, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "                                                shape=(80,80,1), dtype=np.uint8)\n",
        "    def observation(self, obs):\n",
        "        return PreProcessFrame.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "\n",
        "        new_frame = np.reshape(frame, frame.shape).astype(np.float32)\n",
        "\n",
        "        new_frame = 0.299*new_frame[:,:,0] + 0.587*new_frame[:,:,1] + \\\n",
        "                    0.114*new_frame[:,:,2]\n",
        "\n",
        "        new_frame = new_frame[35:195:2, ::2].reshape(80,80,1)\n",
        "\n",
        "        return new_frame.astype(np.uint8)\n",
        "\n",
        "class MoveImgChannel(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(MoveImgChannel, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "                            shape=(self.observation_space.shape[-1],\n",
        "                                   self.observation_space.shape[0],\n",
        "                                   self.observation_space.shape[1]),\n",
        "                            dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class ScaleFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                             env.observation_space.low.repeat(n_steps, axis=0),\n",
        "                             env.observation_space.high.repeat(n_steps, axis=0),\n",
        "                             dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = SkipEnv(env)\n",
        "    env = PreProcessFrame(env)\n",
        "    env = MoveImgChannel(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaleFrame(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH1dwJXz1WeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "77cc7d1a-d8ff-46b0-9389-1387f101e083"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  # the main loop here \n",
        "  env = gym.make('SpaceInvaders-v0')\n",
        "  brain = Agent(gamma= 0.95, epsilon = 1.0, alpha = 0.03, maxMemorysize=5000, replace = None )\n",
        "  while brain.memcntr < brain.memSize:\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # 0 no action, 1 fire, 2 move right, 3 move left, 4 move right fire, 5 move left fire\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            if done and info['ale.lives'] == 0:\n",
        "                reward = -100\n",
        "            brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                                np.mean(observation_[15:200,30:125], axis=2))\n",
        "            observation = observation_\n",
        "  print('done initializing memory')\n",
        "\n",
        "  scores = []\n",
        "  epsHistory = []\n",
        "  numGames = 50\n",
        "  batch_size=32\n",
        "  # uncomment the line below to record every episode.\n",
        "  #env = Wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
        "  for i in range(numGames):\n",
        "      print('starting game ', i+1, 'epsilon: %.4f' % brain.epsilon)\n",
        "      epsHistory.append(brain.epsilon)\n",
        "      done = False\n",
        "      observation = env.reset()\n",
        "      frames = [np.sum(observation[15:200,30:125], axis=2)]\n",
        "      score = 0\n",
        "      lastAction = 0\n",
        "      while not done:\n",
        "          if len(frames) == 3:\n",
        "              action = brain.chooseAction(frames)\n",
        "              frames = []\n",
        "          else:\n",
        "              action = lastAction\n",
        "          observation_, reward, done, info = env.step(action)\n",
        "          score += reward\n",
        "          frames.append(np.sum(observation_[15:200,30:125], axis=2))\n",
        "          if done and info['ale.lives'] == 0:\n",
        "              reward = -100\n",
        "          brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                                np.mean(observation_[15:200,30:125], axis=2))\n",
        "          observation = observation_\n",
        "          brain.learn(batch_size)\n",
        "          lastAction = action\n",
        "          #env.render(\n",
        "      scores.append(score)\n",
        "      print('score:',score)\n",
        "  x = [i+1 for i in range(numGames)]\n",
        "  fileName = str(numGames) + 'Games' + 'Gamma' + str(brain.gamma) + \\\n",
        "              'Alpha' + str(brain.alpha) + 'Memory' + str(brain.memSize)+ '.png'\n",
        "  plotLearning(x, scores, epsHistory, fileName)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done initializing memory\n",
            "starting game  1 epsilon: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d06fc75ca8f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                                 np.mean(observation_[15:200,30:125], axis=2))\n\u001b[1;32m     46\u001b[0m           \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m           \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m           \u001b[0mlastAction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0;31m#env.render(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-b54a78af3e17>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batchsize)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mQtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mQtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxA\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQnext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# for converging to small value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: the derivative for 'indices' is not implemented"
          ]
        }
      ]
    }
  ]
}